"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ —Å–µ—Ç–µ–≤–æ–º —Ç—Ä–∞—Ñ–∏–∫–µ —Å –ø–æ–º–æ—â—å—é ML –º–æ–¥–µ–ª–µ–π.
"""

import pickle
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_auc_score
)
import os


class AnomalyDetector:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ —Å–µ—Ç–µ–≤–æ–º —Ç—Ä–∞—Ñ–∏–∫–µ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Isolation Forest –¥–ª—è unsupervised anomaly detection.
    """
    
    def __init__(self, contamination=0.1, random_state=42):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π.
        
        Args:
            contamination: –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö (0.0-0.5)
            random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.model = IsolationForest(
            contamination=contamination,
            random_state=random_state,
            n_estimators=100
        )
        self.is_trained = False
        self.feature_columns = None  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    
    def train_model(self, csv_file):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º —Ç—Ä–∞—Ñ–∏–∫–µ –∏–∑ CSV —Ñ–∞–π–ª–∞.
        
        Args:
            csv_file: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —Ç—Ä–∞—Ñ–∏–∫–∞
        """
        if not os.path.exists(csv_file):
            raise FileNotFoundError(f"CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_file}")
        
        print(f"üìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {csv_file}")
        df = pd.read_csv(csv_file)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        required_columns = [
            'packet_count', 'total_bytes', 'duration', 'avg_speed',
            'syn_count', 'ack_count', 'fin_count', 'rst_count',
            'protocol', 'src_port', 'dst_port', 'unique_ips'
        ]
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {missing_columns}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫ (is_anomaly=0)
        if 'is_anomaly' in df.columns:
            normal_traffic = df[df['is_anomaly'] == 0].copy()
            print(f"   –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫: {len(normal_traffic)} –∑–∞–ø–∏—Å–µ–π")
            print(f"   –ê–Ω–æ–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è): {len(df) - len(normal_traffic)} –∑–∞–ø–∏—Å–µ–π")
        else:
            normal_traffic = df.copy()
            print(f"   –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ –¥–∞–Ω–Ω—ã–µ: {len(normal_traffic)} –∑–∞–ø–∏—Å–µ–π")
        
        if len(normal_traffic) == 0:
            raise ValueError("–ù–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏!")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏ –º–µ—Ç–∫–∏
        feature_cols = [col for col in required_columns if col in normal_traffic.columns]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª (–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π)
        if 'protocol' in normal_traffic.columns:
            protocol_mapping = {'TCP': 0, 'UDP': 1, 'ICMP': 2, 'OTHER': 3}
            normal_traffic = normal_traffic.copy()
            normal_traffic['protocol_encoded'] = normal_traffic['protocol'].map(
                lambda x: protocol_mapping.get(x, 3)
            )
            feature_cols.remove('protocol')
            feature_cols.append('protocol_encoded')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_train = normal_traffic[feature_cols].values
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        print(f"üîß –û–±—É—á–µ–Ω–∏–µ Isolation Forest –Ω–∞ {len(X_train)} –æ–±—Ä–∞–∑—Ü–∞—Ö...")
        self.model.fit(X_train)
        self.is_trained = True
        self.feature_columns = feature_cols
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")
        print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
        print(f"   Contamination: {self.model.contamination}")
    
    def _prepare_features(self, features):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç DataFrame –≤ –º–∞—Å—Å–∏–≤).
        
        Args:
            features: DataFrame –∏–ª–∏ –º–∞—Å—Å–∏–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        
        Returns:
            numpy array: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        """
        if isinstance(features, pd.DataFrame):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–ª–∏ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            if self.feature_columns:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
                missing_cols = [col for col in self.feature_columns if col not in features.columns]
                if missing_cols:
                    # –ï—Å–ª–∏ –Ω–µ—Ç protocol_encoded, –Ω–æ –µ—Å—Ç—å protocol, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                    if 'protocol' in features.columns and 'protocol_encoded' not in features.columns:
                        protocol_mapping = {'TCP': 0, 'UDP': 1, 'ICMP': 2, 'OTHER': 3}
                        features = features.copy()
                        features['protocol_encoded'] = features['protocol'].map(
                            lambda x: protocol_mapping.get(x, 3)
                        )
                        missing_cols = [col for col in self.feature_columns if col not in features.columns]
                    
                    if missing_cols:
                        raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã: {missing_cols}")
                
                X = features[self.feature_columns].values
            else:
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
                numeric_cols = features.select_dtypes(include=[np.number]).columns.tolist()
                # –ò—Å–∫–ª—é—á–∞–µ–º is_anomaly –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'is_anomaly' in numeric_cols:
                    numeric_cols.remove('is_anomaly')
                X = features[numeric_cols].values
        else:
            X = np.array(features)
        
        return X
    
    def predict(self, features):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            features: DataFrame –∏–ª–∏ –º–∞—Å—Å–∏–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —Ç—Ä–∞—Ñ–∏–∫–∞
        
        Returns:
            array: –ú–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (-1 –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π, 1 –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞)
        """
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train_model().")
        
        X = self._prepare_features(features)
        predictions = self.model.predict(X)
        return predictions
    
    def predict_anomaly_scores(self, features):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç anomaly scores –¥–ª—è –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            features: DataFrame –∏–ª–∏ –º–∞—Å—Å–∏–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —Ç—Ä–∞—Ñ–∏–∫–∞
        
        Returns:
            array: –ú–∞—Å—Å–∏–≤ anomaly scores (–º–µ–Ω—å—à–µ 0 = –∞–Ω–æ–º–∞–ª–∏—è)
        """
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train_model().")
        
        X = self._prepare_features(features)
        scores = self.model.score_samples(X)
        return scores
    
    def predict_proba(self, features):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–π (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏).
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç decision_function –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫.
        
        Args:
            features: DataFrame –∏–ª–∏ –º–∞—Å—Å–∏–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —Ç—Ä–∞—Ñ–∏–∫–∞
        
        Returns:
            array: –ú–∞—Å—Å–∏–≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (—á–µ–º –º–µ–Ω—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º –≤—ã—à–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏)
        """
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train_model().")
        
        X = self._prepare_features(features)
        # decision_function –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
        decision_scores = self.model.decision_function(X)
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1] –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ ROC-AUC
        # –ú–µ–Ω—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è = –±–æ–ª—å—à–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏
        min_score = decision_scores.min()
        max_score = decision_scores.max()
        if max_score - min_score > 0:
            normalized = (decision_scores - min_score) / (max_score - min_score)
            # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º: –º–µ–Ω—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è = –±–æ–ª—å—à–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏
            proba_anomaly = 1 - normalized
        else:
            proba_anomaly = np.zeros_like(decision_scores)
        
        return proba_anomaly
    
    def save_model(self, model_path):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ —Ñ–∞–π–ª.
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train_model().")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        output_dir = os.path.dirname(model_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        model_data = {
            'model': self.model,
            'feature_columns': self.feature_columns,
            'is_trained': self.is_trained
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    def load_model(self, model_path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ —Ñ–∞–π–ª–∞.
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –º–æ–¥–µ–ª—å—é
        """
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (—Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å) –∏ –Ω–æ–≤–æ–≥–æ (—Å–ª–æ–≤–∞—Ä—å)
        if isinstance(model_data, dict):
            self.model = model_data['model']
            self.feature_columns = model_data.get('feature_columns')
            self.is_trained = model_data.get('is_trained', True)
        else:
            self.model = model_data
            self.is_trained = True
        
        print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")


def evaluate_model(test_csv, model):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        test_csv: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å AnomalyDetector
    
    Returns:
        dict: –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (accuracy, precision, recall, f1, confusion_matrix, roc_auc)
    """
    if not model.is_trained:
        raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")
    
    print(f"üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_csv}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv(test_csv)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–∫
    if 'is_anomaly' not in df.columns:
        raise ValueError("–í —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'is_anomaly'")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (0 = –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π, 1 = –∞–Ω–æ–º–∞–ª—å–Ω—ã–π)
    y_true = df['is_anomaly'].values
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (-1 = –∞–Ω–æ–º–∞–ª–∏—è, 1 = –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π)
    y_pred = model.predict(df)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç 0/1 (0 = –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π, 1 = –∞–Ω–æ–º–∞–ª—å–Ω—ã–π)
    # Isolation Forest: -1 = –∞–Ω–æ–º–∞–ª–∏—è, 1 = –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π
    y_pred_binary = (y_pred == -1).astype(int)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_true, y_pred_binary)
    precision = precision_score(y_true, y_pred_binary, zero_division=0)
    recall = recall_score(y_true, y_pred_binary, zero_division=0)
    f1 = f1_score(y_true, y_pred_binary, zero_division=0)
    cm = confusion_matrix(y_true, y_pred_binary)
    
    # ROC-AUC: –∏—Å–ø–æ–ª—å–∑—É–µ–º decision_function –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–π
        y_scores = model.predict_proba(df)
        roc_auc = roc_auc_score(y_true, y_scores)
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å ROC-AUC: {e}")
        roc_auc = None
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n{'='*60}")
    print(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏:")
    print(f"{'='*60}")
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-score:  {f1:.4f}")
    if roc_auc is not None:
        print(f"ROC-AUC:   {roc_auc:.4f}")
    print(f"\nConfusion Matrix:")
    print(f"                Predicted")
    print(f"              Normal  Anomaly")
    print(f"Actual Normal    {cm[0][0]:4d}    {cm[0][1]:4d}")
    print(f"       Anomaly   {cm[1][0]:4d}    {cm[1][1]:4d}")
    print(f"{'='*60}\n")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confusion_matrix': cm,
        'roc_auc': roc_auc,
        'y_true': y_true,
        'y_pred': y_pred_binary,
        'y_scores': y_scores if roc_auc is not None else None
    }
    
    return metrics


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import sys
    
    if len(sys.argv) < 3:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  –û–±—É—á–µ–Ω–∏–µ: python anomaly_detector.py train <train_csv> [model_path]")
        print("  –û—Ü–µ–Ω–∫–∞:   python anomaly_detector.py evaluate <test_csv> <model_path>")
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command == "train":
        train_csv = sys.argv[2]
        model_path = sys.argv[3] if len(sys.argv) > 3 else "models/anomaly_detector.pkl"
        
        detector = AnomalyDetector(contamination=0.1)
        detector.train_model(train_csv)
        detector.save_model(model_path)
    
    elif command == "evaluate":
        if len(sys.argv) < 4:
            print("–û—à–∏–±–∫–∞: –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω—É–∂–Ω—ã test_csv –∏ model_path")
            sys.exit(1)
        
        test_csv = sys.argv[2]
        model_path = sys.argv[3]
        
        detector = AnomalyDetector()
        detector.load_model(model_path)
        metrics = evaluate_model(test_csv, detector)
    
    else:
        print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {command}")
        sys.exit(1)
